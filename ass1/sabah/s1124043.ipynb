{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66f4ed4c",
   "metadata": {},
   "source": [
    "\n",
    "<h1 align=\"center\">Assignment 1</h1>\n",
    "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">Online Tracking and Privacy</h3>    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d88169",
   "metadata": {},
   "source": [
    "- Sabah Serhir Serhir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815901be",
   "metadata": {},
   "source": [
    "#### Step 1: Pick a website from the following list (online pharmacies and shops)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c26a3c",
   "metadata": {},
   "source": [
    "I have chosen a website from Spain www.mediamarkt.es, which is an online shop that sells technological products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d356374f",
   "metadata": {},
   "source": [
    "#### Step 2: Capture the HTTP traffic while accepting cookies (and personal data processing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7bed1e",
   "metadata": {},
   "source": [
    "Done in mediamarkt.es_accept.har"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c07c05",
   "metadata": {},
   "source": [
    "#### Step 3: Capture the HTTP traffic while rejecting cookies (and personal data processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9fa515",
   "metadata": {},
   "source": [
    "Done in mediamarkt.es_reject.har"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec8cb23",
   "metadata": {},
   "source": [
    "#### Step 4: Analyze the HAR Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "784f1d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tldextract is already installed.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\"\"\"\n",
    "This function checks wether tldextract is installed, as it is going to be used, so it does not cause a problem later\n",
    "\"\"\"\n",
    "def check_installation(package):\n",
    "    try:\n",
    "        import importlib\n",
    "        importlib.import_module(package)\n",
    "        print(f\"{package} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"{package} is not installed. Installing...\")\n",
    "        subprocess.check_call(['pip', 'install', package])\n",
    "        \n",
    "check_installation('tldextract')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5e060cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tldextract\n",
    "from datetime import datetime\n",
    "\n",
    "\"\"\"\n",
    "This function loads the har files obtained from the website \n",
    "\"\"\"\n",
    "def load_file(path):\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "\"\"\"\n",
    "This function obtains the entity_name of the organization that owns the domain names of the request urls\n",
    "\"\"\"\n",
    "def extract_entity_name(domain, entity_map):\n",
    "    domain_info = tldextract.extract(domain)\n",
    "    registered_domain = domain_info.registered_domain\n",
    "    return entity_map.get(registered_domain, {\"entityName\": \"Unknown\"})[\"entityName\"]\n",
    "\n",
    "\"\"\"\n",
    "This function obtains the domains of the urls.\n",
    "\n",
    "\"\"\"\n",
    "def extract_domain_info(url):\n",
    "    domain_info = tldextract.extract(url)\n",
    "    return domain_info.registered_domain\n",
    "\n",
    "\"\"\"\n",
    "This function creates a list of dictionaries containing for each request/response the first 128 characters \n",
    "of the request url, url domain, if it is a third party domain, it checks whether it has a set-cookie header\n",
    "not empty and the entity name that owns the domain\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def extract_requests_details_from_har(har_data, entity_map):\n",
    "    main_title = har_data['log']['pages'][0]['title']\n",
    "    website_etld = extract_domain_info(main_title)\n",
    "    requests_list = []\n",
    "\n",
    "    for entry in har_data['log']['entries']:\n",
    "        request_url = entry['request']['url']\n",
    "        url_first_128_char = request_url[:128]\n",
    "        url_domain = extract_domain_info(request_url)\n",
    "        is_third_party = url_domain != website_etld\n",
    "        has_set_cookie = any(\n",
    "            header['name'].lower() == 'set-cookie' for header in entry['response']['headers']\n",
    "        )\n",
    "        entity_name = extract_entity_name(request_url, entity_map)\n",
    "\n",
    "        request_details = {\n",
    "            \"url_first_128_char\": url_first_128_char,\n",
    "            \"url_domain\": url_domain,\n",
    "            \"is_third_party\": is_third_party,\n",
    "            \"has_set_cookie\": has_set_cookie,\n",
    "            \"entity_name\": entity_name\n",
    "        }\n",
    "        requests_list.append(request_details)\n",
    "\n",
    "    return requests_list\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function obtains the total number of requests, the number of requests with a non-empty Cookie header,\n",
    "the number of requests with a non-empty Set-Cookie header, a list of distinct third-party domains, list of \n",
    "distinct domains that set a cookie that can be used for cross-site tracking and a list of distinct entities\n",
    "that own the domain of the request urls\n",
    "\"\"\"\n",
    "def analyze_har_file(har_data, entity_map):\n",
    "    req_non_empty_cookies_count = 0\n",
    "    res_non_empty_cookies_count = 0\n",
    "    third_party_domains = set()\n",
    "    tracker_cookie_domains = set()\n",
    "    third_party_entities = []\n",
    "\n",
    "    main_title = har_data['log']['pages'][0]['title']\n",
    "    main_domain = extract_domain_info(main_title)\n",
    "\n",
    "    for entry in har_data['log']['entries']:\n",
    "        headers_request = entry['request']['headers']\n",
    "        request_cookies = any(header['name'].lower() == 'cookie' for header in headers_request)\n",
    "        if request_cookies:\n",
    "            req_non_empty_cookies_count += 1\n",
    "        headers_response = entry['response']['headers']\n",
    "        response_cookies = any(\n",
    "            header['name'].lower() == 'set-cookie' for header in headers_response\n",
    "        )\n",
    "        if response_cookies:\n",
    "            res_non_empty_cookies_count += 1\n",
    "\n",
    "        url = entry['request']['url']\n",
    "        domain = extract_domain_info(url)\n",
    "        if domain and domain != main_domain and domain not in third_party_domains:\n",
    "            third_party_domains.add(domain)\n",
    "\n",
    "        cookie_response = entry['response'].get('cookies', [])\n",
    "        for cookie in cookie_response:\n",
    "            if cookie.get('sameSite') == 'None' and cookie.get('secure'):\n",
    "                expires_str = cookie.get('expires')\n",
    "                if expires_str:\n",
    "                    expires = datetime.strptime(expires_str, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "                    max_age = (expires - datetime.utcnow()).total_seconds() * 1000 #calculate the difference between the expirer date and actual date\n",
    "                else:\n",
    "                    max_age = cookie.get('maxAge') * 1000 if cookie.get('maxAge') else None\n",
    "                if max_age and max_age >= 60 * 24 * 60 * 60 * 1000: # Verify if it has a minimum lifespan of 60 days converting it to miliseconds\n",
    "                    tracker_cookie_domains.add(cookie.get('domain'))\n",
    "\n",
    "        entity_name = extract_entity_name(url, entity_map)\n",
    "        if entity_name != 'Unknown':\n",
    "            third_party_entities.append(entity_name)\n",
    "\n",
    "    return {\n",
    "        \"num_reqs\": len(har_data['log']['entries']),\n",
    "        \"num_requests_w_cookies\": req_non_empty_cookies_count,\n",
    "        \"num_responses_w_cookies\": res_non_empty_cookies_count,\n",
    "        \"third_party_domains\": list(third_party_domains),\n",
    "        \"tracker_cookie_domains\": list(tracker_cookie_domains),\n",
    "        \"third_party_entities\": list(set(third_party_entities))\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Paths to the HAR files and domain map file\n",
    "    har_path_accept = 'mediamarkt.es_accept.har'\n",
    "    har_path_reject = 'mediamarkt.es_reject.har'\n",
    "    entity_map_path = 'domain_map.json'\n",
    "\n",
    "    # Load domain map\n",
    "    entity_map = load_file(entity_map_path)\n",
    "\n",
    "    # Load HAR data\n",
    "    har_accept = load_file(har_path_accept)\n",
    "    har_reject = load_file(har_path_reject)\n",
    "\n",
    "    # Analyze HAR files\n",
    "    results_accept = analyze_har_file(har_accept, entity_map)\n",
    "    results_reject = analyze_har_file(har_reject, entity_map)\n",
    "\n",
    "    # Extract request/response details from HAR files\n",
    "    requests_details_accept = extract_requests_details_from_har(har_accept, entity_map)\n",
    "    requests_details_reject = extract_requests_details_from_har(har_reject, entity_map)\n",
    "\n",
    "    # Write results to JSON files\n",
    "    with open('mediamarkt.es_accept.json', 'w', encoding='utf-8') as file:\n",
    "        json.dump(results_accept, file, indent=4)\n",
    "        json.dump(requests_details_accept, file, indent=4)\n",
    "\n",
    "    with open('mediamarkt.es_reject.json', 'w', encoding='utf-8') as file:\n",
    "        json.dump(results_reject, file, indent=4)\n",
    "        json.dump(requests_details_reject, file, indent=4)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
